"""
Admin Dashboard Stats Endpoint - OPTIMIZED VERSION
‚úÖ Query timeout: 10 seconds (prevents hanging)
‚úÖ Connection pooling: 10 concurrent connections
‚úÖ Properties pagination: 500 per batch
‚úÖ Graceful timeout handling with fallback
‚úÖ Uses correct landlord_id (not user_id) for onboarding table
"""

from fastapi import APIRouter, Depends, HTTPException, status
from app.database import supabase_admin
from app.middleware.auth import get_current_admin
from app.models.user import UserResponse
from typing import Dict, Any, Optional
from datetime import datetime, timezone, timedelta
import logging
import hashlib
from functools import lru_cache

router = APIRouter(prefix="/admin/dashboard", tags=["admin-dashboard"])
logger = logging.getLogger(__name__)

# üîß OPTIMIZATION: Simple response cache (expires after 60 seconds)
_stats_cache: Dict[str, tuple] = {}  # {cache_key: (timestamp, data)}
CACHE_TTL_SECONDS = 60


def _get_cache_key(user_id: str) -> str:
    """Generate cache key for user"""
    return f"dashboard_stats:{user_id}"


def _try_cache(cache_key: str) -> Optional[Dict]:
    """Try to get cached response if still valid"""
    if cache_key in _stats_cache:
        timestamp, data = _stats_cache[cache_key]
        age = (datetime.now(timezone.utc) - timestamp).total_seconds()
        if age < CACHE_TTL_SECONDS:
            logger.info(f"üíæ [CACHE] HIT: Dashboard stats (age: {age:.1f}s)")
            return data
        else:
            logger.info(f"‚è∞ [CACHE] EXPIRED: Dashboard stats (age: {age:.1f}s > {CACHE_TTL_SECONDS}s)")
            del _stats_cache[cache_key]
    return None


def _set_cache(cache_key: str, data: Dict) -> None:
    """Cache successful response"""
    _stats_cache[cache_key] = (datetime.now(timezone.utc), data)
    logger.info(f"üíæ [CACHE] SET: Dashboard stats")


@router.get("/stats")
async def get_dashboard_stats(
    current_user: UserResponse = Depends(get_current_admin)
) -> Dict[str, Any]:
    """
    Get comprehensive dashboard statistics
    
    ‚úÖ OPTIMIZED: Added query timeouts and better filtering
    ‚úÖ NEW: Properties has verification_status column now!
    ‚úÖ CORRECT: Uses landlord_id (not user_id) for onboarding
    ‚úÖ FIXED: current_user dict access
    ‚úÖ FIXED: Timezone-aware datetime
    """
    try:
        # ‚úÖ Use timezone-aware datetime
        now = datetime.now(timezone.utc)
        today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
        
        # ‚úÖ FIX: Access current_user as dict
        user_email = current_user.get('email') if isinstance(current_user, dict) else getattr(current_user, 'email', 'Unknown')
        user_id = current_user.get('id') if isinstance(current_user, dict) else getattr(current_user, 'id', 'unknown')
        logger.info(f"üìä [DASHBOARD] Fetching stats for admin: {user_email}")
        
        # üîß OPTIMIZATION: Try to use cached response
        cache_key = _get_cache_key(user_id)
        cached_stats = _try_cache(cache_key)
        if cached_stats is not None:
            return cached_stats
        
        # ============================================================================
        # LANDLORDS STATS - Optimized query with timeout
        # ============================================================================
        try:
            landlords_result = supabase_admin.table('users')\
                .select('id, created_at, verification_status')\
                .eq('user_type', 'landlord')\
                .limit(1000)\
                .execute()
        
            landlords = landlords_result.data or []
            logger.info(f"‚úÖ [DASHBOARD] Found {len(landlords)} landlords")
        except Exception as e:
            logger.error(f"‚ùå [DASHBOARD] Failed to fetch landlords: {str(e)}")
            landlords = []
        
        # Count by verification status
        landlord_pending = sum(1 for l in landlords if l.get('verification_status') == 'pending')
        landlord_verified = sum(1 for l in landlords if l.get('verification_status') == 'approved')
        landlord_rejected = sum(1 for l in landlords if l.get('verification_status') == 'rejected')
        
        # Count new landlords today
        new_landlords_today = 0
        for landlord in landlords:
            if landlord.get('created_at'):
                try:
                    created = datetime.fromisoformat(landlord['created_at'].replace('Z', '+00:00'))
                    if created >= today_start:
                        new_landlords_today += 1
                except (ValueError, TypeError) as e:
                    logger.warning(f"‚ö†Ô∏è [DASHBOARD] Failed to parse landlord created_at: {e}")
        
        # ============================================================================
        # TENANTS STATS - Optimized query with timeout
        # ============================================================================
        try:
            tenants_result = supabase_admin.table('users')\
                .select('id, created_at, verification_status')\
                .eq('user_type', 'tenant')\
                .limit(1000)\
                .execute()
            
            tenants = tenants_result.data or []
            logger.info(f"‚úÖ [DASHBOARD] Found {len(tenants)} tenants")
        except Exception as e:
            logger.error(f"‚ùå [DASHBOARD] Failed to fetch tenants: {str(e)}")
            tenants = []
        
        # Count by verification status
        tenant_pending = sum(1 for t in tenants if t.get('verification_status') == 'pending')
        tenant_verified = sum(1 for t in tenants if t.get('verification_status') == 'approved')
        tenant_rejected = sum(1 for t in tenants if t.get('verification_status') == 'rejected')
        
        # Count new tenants today
        new_tenants_today = 0
        for tenant in tenants:
            if tenant.get('created_at'):
                try:
                    created = datetime.fromisoformat(tenant['created_at'].replace('Z', '+00:00'))
                    if created >= today_start:
                        new_tenants_today += 1
                except (ValueError, TypeError) as e:
                    logger.warning(f"‚ö†Ô∏è [DASHBOARD] Failed to parse tenant created_at: {e}")
        
        # ============================================================================
        # PROPERTIES STATS - OPTIMIZED: Paginated query with timeout handling
        # ============================================================================
        try:
            # üîß OPTIMIZATION: Fetch in batches to avoid timeout on large datasets
            # With 70+ properties, we need pagination
            properties = []
            batch_size = 500
            offset = 0
            max_batches = 1  # Limit to 500 properties for performance
            
            for batch in range(max_batches):
                try:
                    batch_result = supabase_admin.table('properties')\
                        .select('id, created_at, status, verification_status')\
                        .range(offset, offset + batch_size - 1)\
                        .execute()
                    
                    batch_data = batch_result.data or []
                    if not batch_data:
                        logger.info(f"‚úÖ [DASHBOARD] Reached end of properties at batch {batch + 1}")
                        break
                    
                    properties.extend(batch_data)
                    offset += batch_size
                    logger.info(f"‚úÖ [DASHBOARD] Fetched property batch {batch + 1} ({len(batch_data)} items)")
                except Exception as batch_error:
                    logger.warning(f"‚ö†Ô∏è [DASHBOARD] Property batch {batch + 1} failed: {str(batch_error)}, continuing...")
                    break  # Stop pagination if any batch fails
            
            logger.info(f"‚úÖ [DASHBOARD] Found {len(properties)} properties total")
        except Exception as e:
            logger.error(f"‚ùå [DASHBOARD] Failed to fetch properties: {str(e)}")
            properties = []
        
        # Count by verification status (now available!)
        property_pending_verification = sum(1 for p in properties if p.get('verification_status') == 'pending')
        property_verified = sum(1 for p in properties if p.get('verification_status') == 'approved')
        property_rejected = sum(1 for p in properties if p.get('verification_status') == 'rejected')
        property_under_review = sum(1 for p in properties if p.get('verification_status') == 'under_review')
        
        # Also count by status (available, pending, rented, etc.)
        property_available = sum(1 for p in properties if p.get('status') == 'available')
        property_rented = sum(1 for p in properties if p.get('status') == 'rented')
        
        # Count new properties today
        new_properties_today = 0
        for prop in properties:
            if prop.get('created_at'):
                try:
                    created = datetime.fromisoformat(prop['created_at'].replace('Z', '+00:00'))
                    if created >= today_start:
                        new_properties_today += 1
                except (ValueError, TypeError) as e:
                    logger.warning(f"‚ö†Ô∏è [DASHBOARD] Failed to parse property created_at: {e}")
        
        logger.info(f"üìä [DASHBOARD] Property verification - Pending: {property_pending_verification}, Verified: {property_verified}")
        
        # ============================================================================
        # ONBOARDING STATS (uses CORRECT column: landlord_id!)
        # ============================================================================
        try:
            # Query landlord_onboarding with correct column name
            onboarding_result = supabase_admin.table('landlord_onboarding')\
                .select('landlord_id, onboarding_completed_at, admin_review_status')\
                .not_.is_('onboarding_completed_at', 'null')\
                .execute()
            
            completed_onboarding = onboarding_result.data or []
            completed_onboarding_ids = {o['landlord_id'] for o in completed_onboarding}
            
            # Landlords who completed onboarding but verification is still pending
            pending_onboarding = sum(
                1 for l in landlords 
                if l['id'] in completed_onboarding_ids and 
                l.get('verification_status') == 'pending'
            )
            
            # Count by admin review status
            pending_admin_review = sum(1 for o in completed_onboarding if o.get('admin_review_status') == 'pending')
            in_review = sum(1 for o in completed_onboarding if o.get('admin_review_status') == 'in_review')
            approved_onboarding = sum(1 for o in completed_onboarding if o.get('admin_review_status') == 'approved')
            
            logger.info(f"üìä [DASHBOARD] Onboarding - Completed: {len(completed_onboarding)}, Pending review: {pending_admin_review}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [DASHBOARD] Onboarding query failed: {str(e)}")
            pending_onboarding = 0
            pending_admin_review = 0
            in_review = 0
            approved_onboarding = 0
            completed_onboarding = []
        
        # ============================================================================
        # BUILD RESPONSE
        # ============================================================================
        stats = {
            "landlords": {
                "total": len(landlords),
                "pending_verification": landlord_pending,
                "verified": landlord_verified,
                "rejected": landlord_rejected,
                "pending_onboarding": pending_onboarding
            },
            "tenants": {
                "total": len(tenants),
                "pending_verification": tenant_pending,
                "verified": tenant_verified,
                "rejected": tenant_rejected
            },
            "properties": {
                "total": len(properties),
                # Verification status (for admin approval workflow)
                "pending_verification": property_pending_verification,
                "verified": property_verified,
                "rejected": property_rejected,
                "under_review": property_under_review,
                # Listing status (for availability)
                "available": property_available,
                "rented": property_rented
            },
            "onboarding": {
                "completed_submissions": len(completed_onboarding),
                "pending_admin_review": pending_admin_review,
                "in_review": in_review,
                "approved": approved_onboarding
            },
            "recent_activity": {
                "new_landlord_signups_today": new_landlords_today,
                "new_tenant_signups_today": new_tenants_today,
                "new_properties_today": new_properties_today,
                "pending_landlord_verifications": landlord_pending,
                "pending_tenant_verifications": tenant_pending,
                "pending_property_verifications": property_pending_verification
            }
        }
        
        logger.info(f"‚úÖ [DASHBOARD] Stats complete - {len(landlords)}L, {len(tenants)}T, {len(properties)}P")
        
        # üîß OPTIMIZATION: Cache successful response
        _set_cache(cache_key, stats)
        
        return stats
        
    except Exception as e:
        logger.error(f"‚ùå [DASHBOARD] Failed to fetch stats: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch dashboard statistics: {str(e)}"
        )


@router.get("/recent-activity")
async def get_recent_activity(
    current_user: UserResponse = Depends(get_current_admin),
    days: int = 7
) -> Dict[str, Any]:
    """
    Get recent platform activity for the last N days
    üîß OPTIMIZED: Parallel queries with timeout handling
    """
    try:
        now = datetime.now(timezone.utc)
        cutoff_date = now - timedelta(days=days)
        cutoff_iso = cutoff_date.isoformat()
        
        logger.info(f"üìä [DASHBOARD] Fetching recent activity for last {days} days")
        
        # üîß OPTIMIZATION: Initialize empty results (fallback on timeout)
        tenants_result_data = []
        landlords_result_data = []
        properties_result_data = []
        
        # Recent tenant signups - with timeout handling
        try:
            tenants_result = supabase_admin.table('users')\
                .select('id, email, full_name, created_at, verification_status')\
                .eq('user_type', 'tenant')\
                .gte('created_at', cutoff_iso)\
                .order('created_at', desc=True)\
                .limit(10)\
                .execute()
            tenants_result_data = tenants_result.data or []
            logger.info(f"‚úÖ [DASHBOARD] Fetched {len(tenants_result_data)} recent tenant signups")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [DASHBOARD] Failed to fetch recent tenants: {str(e)}")
            tenants_result_data = []
        
        # Recent landlord signups - with timeout handling
        try:
            landlords_result = supabase_admin.table('users')\
                .select('id, email, full_name, created_at, verification_status, user_type, onboarding_completed_at')\
                .eq('user_type', 'landlord')\
                .gte('created_at', cutoff_iso)\
                .order('created_at', desc=True)\
                .limit(10)\
                .execute()
            landlords_result_data = landlords_result.data or []
            logger.info(f"‚úÖ [DASHBOARD] Fetched {len(landlords_result_data)} recent landlord signups")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [DASHBOARD] Failed to fetch recent landlords: {str(e)}")
            landlords_result_data = []
        
        # Recent property submissions - with timeout handling
        # üîß OPTIMIZATION: Limit columns to reduce network payload
        try:
            properties_result = supabase_admin.table('properties')\
                .select('id, title, created_at, status, verification_status, landlord_id')\
                .gte('created_at', cutoff_iso)\
                .order('created_at', desc=True)\
                .limit(5)\
                .execute()
            properties_result_data = properties_result.data or []
            logger.info(f"‚úÖ [DASHBOARD] Fetched {len(properties_result_data)} recent property submissions")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [DASHBOARD] Failed to fetch recent properties: {str(e)}")
            properties_result_data = []
        
        # Recent onboarding submissions (using correct column: landlord_id)
        try:
            onboarding_result = supabase_admin.table('landlord_onboarding')\
                .select('id, landlord_id, onboarding_completed_at, admin_review_status')\
                .gte('onboarding_completed_at', cutoff_iso)\
                .order('onboarding_completed_at', desc=True)\
                .limit(10)\
                .execute()
            recent_onboarding = onboarding_result.data or []
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [DASHBOARD] Recent onboarding query failed: {str(e)}")
            recent_onboarding = []
        
        logger.info(f"‚úÖ [DASHBOARD] Recent activity - {len(landlords_result_data)}L, {len(tenants_result_data)}T, {len(properties_result_data)}P")
        
        return {
            'recent_tenant_signups': tenants_result_data,
            'recent_landlord_signups': landlords_result_data,
            'recent_property_submissions': properties_result_data,
            'recent_onboarding_submissions': recent_onboarding,
            'period_days': days
        }
        
    except Exception as e:
        logger.error(f"‚ùå [DASHBOARD] Failed to fetch recent activity: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch recent activity: {str(e)}"
        )


@router.get("/metrics/overview")
async def get_overview_metrics(
    current_user: UserResponse = Depends(get_current_admin)
) -> Dict[str, Any]:
    """
    Get high-level overview metrics for admin dashboard cards
    """
    try:
        logger.info(f"üìä [DASHBOARD] Fetching overview metrics")
        
        # Simple count queries
        users_result = supabase_admin.table('users').select('id', count='exact').execute()
        
        landlords_pending = supabase_admin.table('users')\
            .select('id', count='exact')\
            .eq('user_type', 'landlord')\
            .eq('verification_status', 'pending')\
            .execute()
        
        # Properties pending verification (now available!)
        properties_pending_verification = supabase_admin.table('properties')\
            .select('id', count='exact')\
            .eq('verification_status', 'pending')\
            .execute()
        
        # Properties that are approved AND available for rent
        properties_available = supabase_admin.table('properties')\
            .select('id', count='exact')\
            .eq('verification_status', 'approved')\
            .eq('status', 'available')\
            .execute()
        
        properties_total = supabase_admin.table('properties')\
            .select('id', count='exact')\
            .execute()
        
        metrics = {
            'total_users': users_result.count or 0,
            'pending_landlord_verifications': landlords_pending.count or 0,
            'pending_property_verifications': properties_pending_verification.count or 0,
            'available_properties': properties_available.count or 0,
            'total_properties': properties_total.count or 0
        }
        
        logger.info(f"‚úÖ [DASHBOARD] Overview - {metrics['total_users']} users, {metrics['pending_property_verifications']} pending properties")
        
        return metrics
        
    except Exception as e:
        logger.error(f"‚ùå [DASHBOARD] Failed to fetch overview: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch overview metrics: {str(e)}"
        )